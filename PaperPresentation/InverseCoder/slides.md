---
theme: apple-basic
title: Inverse Coder
---

# Inverse Coder

Improving open source LLM code generation performance without the assistance of Big AI models such as ChatGPT or Claude.

---

## Overview

### InverseInstruct
- Novel approach of training introduced in this paper
- Use a model's own capability of summarizing code to generate

### InverseCoder
- LLMs trained using InverseInstruct

- LLMs perform better at summarizing a formal language, such as programming languages into natural language.
- Adding data generated by itself by converting code to instructions rather than generating code from instructions

---

## Background

### Instruction Tuning LLMs

An LLM without instruction tuning might respond to a prompt of “_teach me how to bake bread_” with “_in a home oven.”_
- This is due to the nature of LLMs predicting the next best token. LLMs only append text.

Achieved by fine-tuning a pre-trained model with a dataset composed of instructions and the supposed outputs.

---

## InverseInstruct

$$
f(\{(x_i,\; y_i)\}) = \{(x^{**}_i, y^{*}_i)\}
$$

### Code Preprocessing

Filter LLM responses that contain markdown code block syntax, which are the triple tick marks

\```

\<Code snippet\>

\```

i.e.

> ```python
> def foo():
>   pass
> ```
> Function `foo` does this

There could be code snippets that aren't within code markers, so text that can pass syntax checking are also collected

---

### Code Preprocessing

Filter out the natural language surrounding the code snippets.

LLMs show examples of how to use the snippet, whether they be functions or classes, so we disregard those and keep the first part.

```json{*}{maxHeight:'400px'}
[
  {
  "Instruction": Create a function to search for a word in an array. The word should be searched in the lowercase version of the array and it should return the index location of its first occurrence.
  word = "test"
  words_array = ["Hello", "World", "Test", "Test", "me"],
  "Response": Here's how you can create the function in Python:

  \```python
  def search_word(word, words_array):
  word = word.lower()
  words_array = [w.lower() for w in words_array]
  if word in words_array:
  return words_array.index(word)
  else:
  return "Word not found"

  word = "test"
  words_array = ["Hello", "World", "Test", "Test", "me"]
  print(search_word(word, words_array))
  \```
  This script turns both the search term and the words in the array to lower-case to ensure the search is case-insensitive. It checks if the word is within the array. If it is, it returns the index of the first occurrence of the word. If not, it returns a "Word not found" message.
  }
]

```

---

````md magic-move
```
Here's how you can create the function in Python:

  ```python
  def search_word(word, words_array):
  word = word.lower()
  words_array = [w.lower() for w in words_array]
  if word in words_array:
    return words_array.index(word)
  else:
    return "Word not found"

  word = "test"
  words_array = ["Hello", "World", "Test", "Test", "me"]
  print(search_word(word, words_array))
  ```

  This script turns both the search term and the words in the array to lower-case to ensure the search is case-insensitive. It checks if the word is within the array. If it is, it returns the index of the first occurrence of the word. If not, it returns a "Word not found" message.
```

```python
  def search_word(word, words_array):
    word = word.lower()
    words_array = [w.lower() for w in words_array]
    if word in words_array:
      return words_array.index(word)
    else:
      return "Word not found"

  word = "test"
  words_array = ["Hello", "World", "Test", "Test", "me"]
  print(search_word(word, words_array))
```


````

- $y_i$: The original trained data
- $y^*_i$: Preprocessed code snippet

---

### Code Summarization

Prompts an instruction-tuned LLM to summarize each pre-processed code snippets alongside multiple new instructions

The prompt provided by the author to the summarizing LLM:
```
You are an exceptionally intelligent coding assistant that consistently delivers accurate and reliable instructions to user responses.

@@ Response
{response}

@@ Instruction
{instruction}
```

---

Code Snippet ($y^*_i$)

```python
def twoSum(nums, target):
    for i in range(len(nums)):
        for j in range(i + 1, len(nums)):
            if nums[i] + nums[j] == target:
                return [i, j]
```

The LLM could return the following instructions ($x^*_{ij}$)

```json
[
"Find the indices of two numbers in the list whose sum is equal to the target value.",
"Return a list containing the indices of two different elements whose sum equals the target.",
"Given an array and a target number, identify two elements such that their addition results in the target and return their indices."
]
```

---

### Self Evaluation and Data Selection

Prompts the same instruction-tuned LLM to select the most appropriate instruction,

resulting in this pair: $\{(x^{**}_i, y^{*}_i)\}$

> **Prompt**: "Does the code contain elements of programming intelligence? Reply with only YES or NO."

The model takes the

<img src="/prob.png" class="h-auto max-w-xs center mx-auto">

---

Sample:
```json
[
  {
    "token": "Yes",
    "logprob": -1.6929036378860474,
    "bytes": [ 52 ]
  },
  {
    "token": "No",
    "logprob": -1.8179036378860474,
    "bytes": [ 55 ]
  },
  {
    "token": "Yes",
    "logprob": -1.8179036378860474,
    "bytes": [ 53 ]
  },
]
```

---

## Training

### Base Models
- CodeLlama-Python-13B
- CodeLlama-Python-7B
- DeepSeek-Coder-Base-6.7B

Trained with the codealpaca-v1 instruction and response dataset

---

### InverseCoder

Train the base models with the dataset generated from the instruction + code InverseInstruct method
Train with the original dataset again

---

## Results

---
